---
layout: post
title: "Notes on Lecture 3: Linear Algebra II"
date: 2018-10-05 01:01:01
categories: [notes]
comments: true
quarter: fa18
---

<style>
  #post h2 { margin: 0.8em 0 0.8em; }
  #post h3 { margin: 1.2em 0 1.2em; }
</style>

Here are some scribed notes from Hao's linear algebra lecture today, which recap a few of the interesting results he mentioned. As a standard disclaimer, this isn't a proper substitute for attending/watching lecture yourself or taking your own notes – for instance, I didn't bother to write down everything, nor did I address the justifications for many ideas.

<a href="{{ site.url }}/notes/fa18/lec03.pdf" class="btn btn-warning" style="line-height: 1.2; padding: 8px 10px">lec03.pdf</a>&nbsp;&nbsp; <em>reproduced below for convenience</em>

<!--more-->

<h2 id="pseudoinverse">Pseudoinverse</h2>
<p>Say we want to solve the matrix equation <span class="math inline">\(\mathbf{Ax} = \mathbf{b}\)</span>. <span class="math display">\[\begin{bmatrix}
a_{11} &amp; \ldots &amp; a_{1n} \\
\vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; \ldots &amp; a_{mn}
\end{bmatrix}
\begin{bmatrix}
x_1 \\
\vdots \\
x_n
\end{bmatrix}
=
\begin{bmatrix}
b_1 \\
\vdots \\
b_m
\end{bmatrix}\]</span> We can look at this as solving the system of linear equations <span class="math display">\[\begin{gathered}
a_{11}x_1 + \ldots + a_{1n}x_n = b_1 \\
\vdots \\
a_{m1}x_1 + \ldots + a_{mn}x_n = b_m\end{gathered}\]</span> which can be overdetermined, exactly determined, or underdetermined, according to the number of equations <span class="math inline">\(m\)</span> versus the number of unknowns <span class="math inline">\(n\)</span>. By extension, the system can have zero, one, or infinitely many solutions.</p>
<p>We call <span class="math inline">\(\mathbf{A}^\dagger\)</span> the Moore-Penrose inverse, or <strong>pseudoinverse</strong>, of <span class="math inline">\(\mathbf{A}\)</span>.</p>
<ul>
<li>If the system has no solutions, <span class="math inline">\(\mathbf{x} = \mathbf{A^\dagger b}\)</span> is a least-squares solution <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> to <span class="math inline">\(\mathbf{Ax} = \mathbf{b}\)</span>.</li>
<li>If the system has one solution, <span class="math inline">\(\mathbf{x} = \mathbf{A^\dagger b} = \mathbf{A^{-1} b}\)</span> is the one solution to <span class="math inline">\(\mathbf{Ax} = \mathbf{b}\)</span>.</li>
<li>If the system has infinitely many solutions, <span class="math inline">\(\mathbf{x} = \mathbf{A^\dagger b}\)</span> is the least-norm solution <a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> to <span class="math inline">\(\mathbf{Ax} = \mathbf{b}\)</span>.</li>
</ul>
<p>Hence we can use the pseudoinverse to solve any matrix equation.</p>
<h2 id="matrix-rank">Matrix Rank</h2>
<p>The rank of a matrix <span class="math inline">\(\mathbf{A}\)</span> tells us the dimension of its image. If <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span> has rank <span class="math inline">\(k\)</span>, then it maps all of <span class="math inline">\(\mathbb{R}^n\)</span> to a <span class="math inline">\(k\)</span>-dimensional subspace. For example, if it were rank 1 it would map <span class="math inline">\(\mathbb{R}^n\)</span> to a line; if it were rank 2 it would map <span class="math inline">\(\mathbb{R}^n\)</span> to a plane.</p>
<p>To see this, we can consider <span class="math inline">\(\mathbf{A}\)</span> in terms of its column vectors. Then <span class="math display">\[\begin{bmatrix}
\vert &amp; \ldots &amp; \vert \\
\mathbf{a}_1 &amp; \ldots &amp; \mathbf{a}_n \\
\vert &amp; \ldots &amp; \vert
\end{bmatrix}
\begin{bmatrix}
x_1 \\
\vdots \\
x_n
\end{bmatrix}
= x_1\mathbf{a}_1 + \ldots + x_n\mathbf{a}_n\]</span> i.e. each output is a linear combination of the column vectors <span class="math inline">\(\mathbf{a}_1, \ldots, \mathbf{a}_n\)</span>. Then, for instance if the matrix is rank <span class="math inline">\(1\)</span>, we are always linearly combining vectors which are scales of each other – which of course can only produce a line!</p>
<p>It's kind of definitional that the rank is the dimension of the column space, but still worth registering.</p>
<h2 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h2>
<p>A few notes and definitions:</p>
<ul>
<li>An eigenvector of <span class="math inline">\(\mathbf{A}\)</span> is a vector which retains its direction when <span class="math inline">\(\mathbf{A}\)</span> is applied to it.</li>
<li>The trace, determinant, and rank of <span class="math inline">\(\mathbf{A}\)</span> can all be defined in terms of its eigenvalues.</li>
<li>The <strong>spectrum</strong> of <span class="math inline">\(\mathbf{A}\)</span> is the set of all of its eigenvalues.</li>
<li>The <strong>spectral radius</strong> of <span class="math inline">\(\mathbf{A}\)</span> is its maximum eigenvalue according to magnitude.</li>
</ul>
<h3 id="eigendecomposition">Eigendecomposition</h3>
<p>If an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span> has <span class="math inline">\(n\)</span> linearly independent eigenvectors, it is diagonalizable and can be written in the form <span class="math inline">\(\mathbf{A} = \mathbf{VDV^{-1}}\)</span>, where <span class="math inline">\(\mathbf{V}\)</span> is a matrix with the eigenvectors <span class="math inline">\(\mathbf{v}_i\)</span> as columns and <span class="math inline">\(\mathbf{D}\)</span> is a diagonal matrix with the eigenvalues <span class="math inline">\(\lambda_i\)</span> as entries (ordered in the same way as their corresponding eigenvectors).</p>
<p>Why? Because <span class="math display">\[\begin{aligned}
\mathbf{AV}
=
\begin{bmatrix}
\vert &amp; \ldots &amp; \vert \\
\mathbf{Av}_1 &amp; \ldots &amp; \mathbf{Av}_n \\
\vert &amp; \ldots &amp; \vert
\end{bmatrix}
=
\begin{bmatrix}
\vert &amp; \ldots &amp; \vert \\
\lambda_1\mathbf{v}_1 &amp; \ldots &amp; \lambda_n\mathbf{v}_n \\
\vert &amp; \ldots &amp; \vert
\end{bmatrix}
=
\begin{bmatrix}
\vert &amp; \ldots &amp; \vert \\
\mathbf{v}_1 &amp; \ldots &amp; \mathbf{v}_n \\
\vert &amp; \ldots &amp; \vert
\end{bmatrix}
\begin{bmatrix}
\lambda_1 &amp; \ldots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \ldots &amp; \lambda_n
\end{bmatrix}
=
\mathbf{VD}\end{aligned}\]</span> and therefore <span class="math inline">\(\mathbf{A} = \mathbf{VDV^{-1}}\)</span>.</p>
<p>(Spectral theorem.) If a real <span class="math inline">\(n \times n\)</span> matrix is symmetric, it is furthermore <em>orthogonally diagonalizable</em> and has <span class="math inline">\(n\)</span> mutually orthogonal eigenvectors with real eigenvalues. We can thus construct an orthogonal matrix <span class="math inline">\(\mathbf{V}\)</span> from these eigenvectors, and have the result that <span class="math display">\[\mathbf{A} = \mathbf{VDV^{-1}} = \mathbf{VDV^T}\]</span></p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p><span class="math inline">\(\arg\min_{\mathbf{x}} \|\mathbf{Ax} - \mathbf{b}\|_2^2\)</span>&nbsp; <a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p><span class="math inline">\(\arg\min_{\mathbf{x}} \|\mathbf{x}\|_2\)</span> s.t. <span class="math inline">\(\mathbf{Ax} = \mathbf{b}\)</span>&nbsp; <a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</section>
